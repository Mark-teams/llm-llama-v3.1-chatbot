{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sudo: apr-get: command not found\n",
      ">>> Downloading ollama...\n",
      "######################################################################## 100.0%#=#=-#  #                                                                                                               34.3%\n",
      ">>> Installing ollama to /usr/local/bin...\n",
      ">>> Creating ollama user...\n",
      ">>> Adding ollama user to video group...\n",
      ">>> Adding current user to ollama group...\n",
      ">>> Creating ollama systemd service...\n",
      "WARNING: Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n",
      ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
      ">>> Install complete. Run \"ollama\" from the command line.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find '/home/codespace/.ollama/id_ed25519'. Generating new private key.\n",
      "Your new public key is: \n",
      "\n",
      "ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAILgD467nEBdYt+NKYtMm1G7kuCu09g5pRdcjw9aCAjPE\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/07/31 02:16:40 routes.go:1099: INFO server config env=\"map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_HOST:http://0.0.0.0:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_LLM_LIBRARY: OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/codespace/.ollama/models OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:0 OLLAMA_ORIGINS:[* http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://*] OLLAMA_RUNNERS_DIR: OLLAMA_SCHED_SPREAD:false OLLAMA_TMPDIR: ROCR_VISIBLE_DEVICES:]\"\n",
      "time=2024-07-31T02:16:40.563Z level=INFO source=images.go:784 msg=\"total blobs: 0\"\n",
      "time=2024-07-31T02:16:40.563Z level=INFO source=images.go:791 msg=\"total unused blobs removed: 0\"\n",
      "time=2024-07-31T02:16:40.564Z level=INFO source=routes.go:1146 msg=\"Listening on [::]:11434 (version 0.3.0)\"\n",
      "time=2024-07-31T02:16:40.564Z level=INFO source=payload.go:30 msg=\"extracting embedded files\" dir=/tmp/ollama2541654620/runners\n",
      "time=2024-07-31T02:16:45.618Z level=INFO source=payload.go:44 msg=\"Dynamic LLM libraries [cpu cpu_avx cpu_avx2 cuda_v11 rocm_v60102]\"\n",
      "time=2024-07-31T02:16:45.618Z level=INFO source=gpu.go:205 msg=\"looking for compatible GPUs\"\n",
      "time=2024-07-31T02:16:45.629Z level=INFO source=gpu.go:346 msg=\"no compatible GPUs were discovered\"\n",
      "time=2024-07-31T02:16:45.629Z level=INFO source=types.go:105 msg=\"inference compute\" id=0 library=cpu compute=\"\" driver=0.0 name=\"\" total=\"15.6 GiB\" available=\"13.4 GiB\"\n"
     ]
    }
   ],
   "source": [
    "!sudo apr-get install -y pciutils\n",
    "!curl -fsSL https://ollama.com/install.sh | sh\n",
    "from IPython.display import clear_output\n",
    "\n",
    "####\n",
    "import os\n",
    "import threading\n",
    "import subprocess\n",
    "import requests\n",
    "import json\n",
    "\n",
    "def ollama():\n",
    "  os.environ['OLLAMA_HOST']='0.0.0.0:11434'\n",
    "  os.environ['OLLAMA_ORIGINS']='*'\n",
    "  subprocess.Popen([\"ollama\",\"serve\"])\n",
    "\n",
    "ollama_thread=threading.Thread(target=ollama)\n",
    "ollama_thread.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "!ollama pull llama3.1:8b\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lightrag[ollama]\n",
      "  Downloading lightrag-0.1.0b6-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting backoff<3.0.0,>=2.2.1 (from lightrag[ollama])\n",
      "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: jinja2<4.0.0,>=3.1.3 in /home/codespace/.local/lib/python3.10/site-packages (from lightrag[ollama]) (3.1.4)\n",
      "Collecting jsonlines<5.0.0,>=4.0.0 (from lightrag[ollama])\n",
      "  Downloading jsonlines-4.0.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.6.0 in /home/codespace/.local/lib/python3.10/site-packages (from lightrag[ollama]) (1.6.0)\n",
      "Collecting numpy<2.0.0,>=1.26.4 (from lightrag[ollama])\n",
      "  Downloading numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m671.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting python-dotenv<2.0.0,>=1.0.1 (from lightrag[ollama])\n",
      "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=6.0.1 in /home/codespace/.local/lib/python3.10/site-packages (from lightrag[ollama]) (6.0.1)\n",
      "Collecting tiktoken<0.8.0,>=0.7.0 (from lightrag[ollama])\n",
      "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.4 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from lightrag[ollama]) (4.66.4)\n",
      "Collecting ollama<0.3.0,>=0.2.1 (from lightrag[ollama])\n",
      "  Downloading ollama-0.2.1-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/codespace/.local/lib/python3.10/site-packages (from jinja2<4.0.0,>=3.1.3->lightrag[ollama]) (2.1.5)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /home/codespace/.local/lib/python3.10/site-packages (from jsonlines<5.0.0,>=4.0.0->lightrag[ollama]) (23.2.0)\n",
      "Requirement already satisfied: httpx<0.28.0,>=0.27.0 in /home/codespace/.local/lib/python3.10/site-packages (from ollama<0.3.0,>=0.2.1->lightrag[ollama]) (0.27.0)\n",
      "Collecting regex>=2022.1.18 (from tiktoken<0.8.0,>=0.7.0->lightrag[ollama])\n",
      "  Downloading regex-2024.7.24-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.5/40.5 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests>=2.26.0 in /home/codespace/.local/lib/python3.10/site-packages (from tiktoken<0.8.0,>=0.7.0->lightrag[ollama]) (2.32.3)\n",
      "Requirement already satisfied: anyio in /home/codespace/.local/lib/python3.10/site-packages (from httpx<0.28.0,>=0.27.0->ollama<0.3.0,>=0.2.1->lightrag[ollama]) (4.4.0)\n",
      "Requirement already satisfied: certifi in /home/codespace/.local/lib/python3.10/site-packages (from httpx<0.28.0,>=0.27.0->ollama<0.3.0,>=0.2.1->lightrag[ollama]) (2024.7.4)\n",
      "Requirement already satisfied: httpcore==1.* in /home/codespace/.local/lib/python3.10/site-packages (from httpx<0.28.0,>=0.27.0->ollama<0.3.0,>=0.2.1->lightrag[ollama]) (1.0.5)\n",
      "Requirement already satisfied: idna in /home/codespace/.local/lib/python3.10/site-packages (from httpx<0.28.0,>=0.27.0->ollama<0.3.0,>=0.2.1->lightrag[ollama]) (3.7)\n",
      "Requirement already satisfied: sniffio in /home/codespace/.local/lib/python3.10/site-packages (from httpx<0.28.0,>=0.27.0->ollama<0.3.0,>=0.2.1->lightrag[ollama]) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/codespace/.local/lib/python3.10/site-packages (from httpcore==1.*->httpx<0.28.0,>=0.27.0->ollama<0.3.0,>=0.2.1->lightrag[ollama]) (0.14.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.local/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken<0.8.0,>=0.7.0->lightrag[ollama]) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken<0.8.0,>=0.7.0->lightrag[ollama]) (2.0.7)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/codespace/.local/lib/python3.10/site-packages (from anyio->httpx<0.28.0,>=0.27.0->ollama<0.3.0,>=0.2.1->lightrag[ollama]) (1.2.1)\n",
      "Requirement already satisfied: typing-extensions>=4.1 in /home/codespace/.local/lib/python3.10/site-packages (from anyio->httpx<0.28.0,>=0.27.0->ollama<0.3.0,>=0.2.1->lightrag[ollama]) (4.12.2)\n",
      "Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
      "Downloading jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\n",
      "Downloading numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m37.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading ollama-0.2.1-py3-none-any.whl (9.7 kB)\n",
      "Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading lightrag-0.1.0b6-py3-none-any.whl (159 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m159.1/159.1 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading regex-2024.7.24-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (776 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m776.5/776.5 kB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: regex, python-dotenv, numpy, jsonlines, backoff, tiktoken, ollama, lightrag\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.0.0\n",
      "    Uninstalling numpy-2.0.0:\n",
      "      Successfully uninstalled numpy-2.0.0\n",
      "Successfully installed backoff-2.2.1 jsonlines-4.0.0 lightrag-0.1.0b6 numpy-1.26.4 ollama-0.2.1 python-dotenv-1.0.1 regex-2024.7.24 tiktoken-0.7.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -U lightrag[ollama]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightrag.core.generator import Generator\n",
    "from lightrag.core.component import Component\n",
    "from lightrag.core.model_client import ModelClient\n",
    "from lightrag.components.model_client import OllamaClient,GroqAPIClient\n",
    "\n",
    "import time\n",
    "qa_template = r\"\"\"<SYS>\n",
    "you are a helpful assistant.\n",
    "</Sys>\n",
    "user: {{input_str}}\n",
    "You:\"\"\"\n",
    "\n",
    "class SimpleQA(Component):\n",
    "  def __init__(self,model_client:ModelClient, model_kwargs:dict):\n",
    "    super().__init__()\n",
    "    self.generator = Generator(\n",
    "        model_client=model_client,\n",
    "        model_kwargs=model_kwargs,\n",
    "        template=qa_template,\n",
    "    )\n",
    "  def call(self,input:dict)->str:\n",
    "    return self.generator.call({\"input_str\":str(input)})\n",
    "  async def acall(self,input:dict)->str:\n",
    "    return await self.generator.acall({\"input_str\":str(input)})\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "time=2024-07-31T02:20:15.265Z level=INFO source=memory.go:309 msg=\"offload to cpu\" layers.requested=-1 layers.model=33 layers.offload=0 layers.split=\"\" memory.available=\"[13.3 GiB]\" memory.required.full=\"5.8 GiB\" memory.required.partial=\"0 B\" memory.required.kv=\"1.0 GiB\" memory.required.allocations=\"[5.8 GiB]\" memory.weights.total=\"4.7 GiB\" memory.weights.repeating=\"4.3 GiB\" memory.weights.nonrepeating=\"411.0 MiB\" memory.graph.full=\"560.0 MiB\" memory.graph.partial=\"677.5 MiB\"\n",
      "time=2024-07-31T02:20:15.266Z level=INFO source=server.go:383 msg=\"starting llama server\" cmd=\"/tmp/ollama2541654620/runners/cpu_avx2/ollama_llama_server --model /home/codespace/.ollama/models/blobs/sha256-87048bcd55216712ef14c11c2c303728463207b165bf18440b9b84b07ec00f87 --ctx-size 8192 --batch-size 512 --embedding --log-disable --no-mmap --parallel 4 --port 34053\"\n",
      "time=2024-07-31T02:20:15.266Z level=INFO source=sched.go:437 msg=\"loaded runners\" count=1\n",
      "time=2024-07-31T02:20:15.266Z level=INFO source=server.go:583 msg=\"waiting for llama runner to start responding\"\n",
      "time=2024-07-31T02:20:15.267Z level=INFO source=server.go:617 msg=\"waiting for server to become available\" status=\"llm server error\"\n",
      "llama_model_loader: loaded meta data with 29 key-value pairs and 291 tensors from /home/codespace/.ollama/models/blobs/sha256-87048bcd55216712ef14c11c2c303728463207b165bf18440b9b84b07ec00f87 (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Meta Llama 3.1 8B Instruct\n",
      "llama_model_loader: - kv   3:                           general.finetune str              = Instruct\n",
      "llama_model_loader: - kv   4:                           general.basename str              = Meta-Llama-3.1\n",
      "llama_model_loader: - kv   5:                         general.size_label str              = 8B\n",
      "llama_model_loader: - kv   6:                            general.license str              = llama3.1\n",
      "llama_model_loader: - kv   7:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\n",
      "llama_model_loader: - kv   8:                          general.languages arr[str,8]       = [\"en\", \"de\", \"fr\", \"it\", \"pt\", \"hi\", ...\n",
      "llama_model_loader: - kv   9:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv  10:                       llama.context_length u32              = 131072\n",
      "llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  17:                          general.file_type u32              = 2\n",
      "llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256\n",
      "llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
      "llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009\n",
      "llama_model_loader: - kv  27:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...\n",
      "llama_model_loader: - kv  28:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_0:  225 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO [main] build info | build=1 commit=\"d94c6e0\" tid=\"126785078486912\" timestamp=1722392415\n",
      "INFO [main] system info | n_threads=2 n_threads_batch=-1 system_info=\"AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 0 | \" tid=\"126785078486912\" timestamp=1722392415 total_threads=4\n",
      "INFO [main] HTTP server listening | hostname=\"127.0.0.1\" n_threads_http=\"6\" port=\"34053\" tid=\"126785078486912\" timestamp=1722392415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "time=2024-07-31T02:20:15.517Z level=INFO source=server.go:617 msg=\"waiting for server to become available\" status=\"llm server loading model\"\n",
      "llm_load_vocab: special tokens cache size = 256\n",
      "llm_load_vocab: token to piece cache size = 0.7999 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 128256\n",
      "llm_load_print_meta: n_merges         = 280147\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 131072\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 500000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 131072\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 8B\n",
      "llm_load_print_meta: model ftype      = Q4_0\n",
      "llm_load_print_meta: model params     = 8.03 B\n",
      "llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) \n",
      "llm_load_print_meta: general.name     = Meta Llama 3.1 8B Instruct\n",
      "llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\n",
      "llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: LF token         = 128 'Ä'\n",
      "llm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: max token length = 256\n",
      "llm_load_tensors: ggml ctx size =    0.14 MiB\n",
      "llm_load_tensors:        CPU buffer size =  4437.80 MiB\n",
      "llama_new_context_with_model: n_ctx      = 8192\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 500000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =  1024.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     2.02 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   560.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO [main] model loaded | tid=\"126785078486912\" timestamp=1722392429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "time=2024-07-31T02:20:29.818Z level=INFO source=server.go:622 msg=\"llama runner started in 14.55 seconds\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GIN] 2024/07/31 - 02:21:40 | 200 |         1m25s |       127.0.0.1 | POST     \"/api/generate\"\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Answer:** What a wonderful topic!\n",
       "\n",
       "Happiness can be defined in many ways, but at its core, it's a positive emotional state that brings joy and fulfillment to one's life. It's the feeling of being content, satisfied, and even blissful.\n",
       "\n",
       "Research has identified various factors that contribute to happiness, such as:\n",
       "\n",
       "1. **Positive relationships**: Strong connections with loved ones, friends, and community.\n",
       "2. **Gratitude**: Practicing appreciation for the good things in life.\n",
       "3. **Physical health**: Engaging in regular exercise, healthy eating, and adequate sleep.\n",
       "4. **Meaningful activities**: Pursuing hobbies, passions, and work that bring purpose and fulfillment.\n",
       "5. **Resilience**: Developing coping skills to navigate life's challenges with optimism and adaptability.\n",
       "\n",
       "According to the Harvard Happiness Study (1938-2012), people who reported being happy were more likely to:\n",
       "\n",
       "* Live longer\n",
       "* Have stronger immune systems\n",
       "* Experience better mental health\n",
       "* Have healthier relationships\n",
       "* Enjoy greater overall well-being\n",
       "\n",
       "While happiness can vary from person to person, it's clear that cultivating a positive mindset and focusing on the things that bring joy and fulfillment are essential for achieving true happiness.\n",
       "\n",
       "Would you like me to explore any specific aspect of happiness further?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from lightrag.components.model_client import OllamaClient\n",
    "from IPython.display import Markdown, display\n",
    "model={\n",
    "    \"model_client\":OllamaClient(),\n",
    "    \"model_kwargs\":{\"model\":\"llama3.1:8b\"}\n",
    "}\n",
    "qa=SimpleQA(**model)\n",
    "output=qa(\"what is happiness\")\n",
    "display(Markdown(f\"**Answer:** {output.data}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5000\n",
      "Press CTRL+C to quit\n",
      "127.0.0.1 - - [31/Jul/2024 05:43:42] \"GET / HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "# !pip install flask\n",
    "# %pip install jsonify\n",
    "# import flast module\n",
    "from flask import Flask, render_template,jsonify,request\n",
    "\n",
    "# instance of flask application\n",
    "app = Flask(__name__)\n",
    " \n",
    "# home route that returns below text when root url is accessed\n",
    "@app.route(\"/\")\n",
    "def home():\n",
    "   return render_template(\"index.html\")\n",
    "\n",
    "@app.route(\"/llm_run\",methods=[\"POST\",\"GET\"])\n",
    "def llm_run():\n",
    "    prompt=None\n",
    "    print(prompt,request.method)\n",
    "    if request.method ==\"POST\":\n",
    "       print(\"Value\",request.form)\n",
    "       prompt= request.form[\"search\"]\n",
    "       print(str(prompt))\n",
    "       output=qa(prompt)\n",
    "       print(\"running....\")\n",
    "    # display(Markdown(f\"**Answer:** {output.data}\")) str(output.data)\n",
    "    return jsonify({'htmlresponse': render_template('response.html',resp=output)})\n",
    " \n",
    "if __name__ == '__main__':  \n",
    "   app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Answer:** The answer to 2 + 2 is 4. Is there anything else I can help you with?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output=qa(\"what is 2+2\")\n",
    "clear_output()\n",
    "display(Markdown(f\"**Answer:** {output.data}\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
